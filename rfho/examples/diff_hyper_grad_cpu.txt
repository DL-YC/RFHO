Differences of hypergradient computations between forward and backward-HG for CPU execution
(there are basically no differences here...)
# run on all_methods_on_mnist.py (_check_all_methods)

# first run

SAVE DICT:
step: 0
mode: forward
test accuracy: 0.636214
validation accuracy: 0.633857
training accuracy: 0.55
validation error: 2.06643
memory usage (mb): 22.57827553215
weights: [-0.01705388 -0.02053495 -0.00783848 ...,  0.01338695 -0.01207654
  0.00042103]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 11.980854]
rho_l1_1: [0.0, 10.473064]
rho_l1_2: [0.0, 33.517784]
rho_l2_0: [0.0, 1.2106586]
rho_l2_1: [0.0, 1.4387809]
rho_l2_2: [0.0, 0.48373219]
eta: [0.010999993, -39.794144]
mu: [0.50099999, -0.76688409]
Elapsed time (sec): 49

SAVE DICT:
step: 0
mode: reverse
test accuracy: 0.636214
validation accuracy: 0.633857
training accuracy: 0.55
validation error: 2.06643
memory usage (mb): 255.88374289122
weights: [-0.01705388 -0.02053495 -0.00783848 ...,  0.01338695 -0.01207654
  0.00042103]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 11.980883]
rho_l1_1: [0.0, 10.473075]
rho_l1_2: [0.0, 33.517853]
rho_l2_0: [0.0, 1.2106622]
rho_l2_1: [0.0, 1.4387833]
rho_l2_2: [0.0, 0.48373321]
eta: [0.010999993, -39.794281]
mu: [0.50099999, -0.76688707]
Elapsed time (sec): 15




# second run....

step: 0
mode: forward
test accuracy: 0.636214
validation accuracy: 0.633857
training accuracy: 0.55
validation error: 2.06643
memory usage (mb): 22.57827553215
weights: [-0.01705388 -0.02053495 -0.00783848 ...,  0.01338695 -0.01207654
  0.00042103]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 11.980854]
rho_l1_1: [0.0, 10.473064]
rho_l1_2: [0.0, 33.517784]
rho_l2_0: [0.0, 1.2106586]
rho_l2_1: [0.0, 1.4387809]
rho_l2_2: [0.0, 0.48373219]
eta: [0.010999993, -39.794144]
mu: [0.50099999, -0.76688409]
Elapsed time (sec): 47

SAVE DICT:
step: 0
mode: reverse
test accuracy: 0.636214
validation accuracy: 0.633857
training accuracy: 0.55
validation error: 2.06643
memory usage (mb): 255.88374289122
weights: [-0.01705388 -0.02053495 -0.00783848 ...,  0.01338695 -0.01207654
  0.00042103]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 11.980883]
rho_l1_1: [0.0, 10.473075]
rho_l1_2: [0.0, 33.517853]
rho_l2_0: [0.0, 1.2106622]
rho_l2_1: [0.0, 1.4387833]
rho_l2_2: [0.0, 0.48373321]
eta: [0.010999993, -39.794281]
mu: [0.50099999, -0.76688707]
Elapsed time (sec): 15


# third run.... 
with hyperparameter optimization




SAVE DICT:
step: 0
mode: forward
test accuracy: 0.636214
validation accuracy: 0.633857
training accuracy: 0.55
validation error: 2.06643
memory usage (mb): 22.57827553215
weights: [-0.01705388 -0.02053495 -0.00783848 ...,  0.01338695 -0.01207654
  0.00042103]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 11.980854]
rho_l1_1: [0.0, 10.473064]
rho_l1_2: [0.0, 33.517784]
rho_l2_0: [0.0, 1.2106586]
rho_l2_1: [0.0, 1.4387809]
rho_l2_2: [0.0, 0.48373219]
eta: [0.010999993, -39.794144]
mu: [0.50099999, -0.76688409]
Elapsed time (sec): 46

SAVE DICT:
step: 1
mode: forward
test accuracy: 0.556
validation accuracy: 0.561238
training accuracy: 0.545
validation error: 2.08558
memory usage (mb): 22.57827553215
weights: [ 0.00110424 -0.01956838 -0.04185919 ...,  0.01046236 -0.01378959
  0.00010357]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 11.590072]
rho_l1_1: [0.0, 10.83959]
rho_l1_2: [0.0, 34.790379]
rho_l2_0: [0.0, 1.2212617]
rho_l2_1: [0.0, 1.4819916]
rho_l2_2: [0.0, 0.49600434]
eta: [0.011994326, -34.834724]
mu: [0.50199884, -0.73887223]
Elapsed time (sec): 84

SAVE DICT:
step: 2
mode: forward
test accuracy: 0.600214
validation accuracy: 0.595857
training accuracy: 0.505
validation error: 2.04999
memory usage (mb): 22.57827553215
weights: [-0.05886209  0.01617052  0.00187904 ...,  0.0177014  -0.01881725
  0.0042787 ]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 13.679873]
rho_l1_1: [0.0, 14.836711]
rho_l1_2: [0.0, 41.68634]
rho_l2_0: [0.0, 1.5119481]
rho_l2_1: [0.0, 1.9328388]
rho_l2_2: [0.0, 0.6364314]
eta: [0.012992319, -39.32436]
mu: [0.50300086, -0.90952623]
Elapsed time (sec): 122


  reverse

SAVE DICT:
step: 0
mode: reverse
test accuracy: 0.636214
validation accuracy: 0.633857
training accuracy: 0.55
validation error: 2.06643
memory usage (mb): 255.88374289122
weights: [-0.01705388 -0.02053495 -0.00783848 ...,  0.01338695 -0.01207654
  0.00042103]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 11.980883]
rho_l1_1: [0.0, 10.473075]
rho_l1_2: [0.0, 33.517853]
rho_l2_0: [0.0, 1.2106622]
rho_l2_1: [0.0, 1.4387833]
rho_l2_2: [0.0, 0.48373321]
eta: [0.010999993, -39.794281]
mu: [0.50099999, -0.76688707]
Elapsed time (sec): 15

SAVE DICT:
step: 1
mode: reverse
test accuracy: 0.556
validation accuracy: 0.561238
training accuracy: 0.545
validation error: 2.08558
memory usage (mb): 255.88374289122
weights: [ 0.00110424 -0.01956838 -0.04185919 ...,  0.01046236 -0.01378959
  0.00010357]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 11.590105]
rho_l1_1: [0.0, 10.839594]
rho_l1_2: [0.0, 34.789242]
rho_l2_0: [0.0, 1.2212667]
rho_l2_1: [0.0, 1.4819945]
rho_l2_2: [0.0, 0.49600551]
eta: [0.011994326, -34.834846]
mu: [0.50199884, -0.73887497]
Elapsed time (sec): 29

SAVE DICT:
step: 2
mode: reverse
test accuracy: 0.600214
validation accuracy: 0.595857
training accuracy: 0.505
validation error: 2.04999
memory usage (mb): 255.88374289122
weights: [-0.05886209  0.01617052  0.00187904 ...,  0.0177014  -0.01881725
  0.0042787 ]
# weights: 328810
# hyperparameters: 8
# iterations: 100
rho_l1_0: [0.0, 13.679888]
rho_l1_1: [0.0, 14.836729]
rho_l1_2: [0.0, 41.68644]
rho_l2_0: [0.0, 1.5119534]
rho_l2_1: [0.0, 1.932842]
rho_l2_2: [0.0, 0.63643342]
eta: [0.012992319, -39.324543]
mu: [0.50300086, -0.90953016]
Elapsed time (sec): 42
