{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import rfho as rf\n",
    "\n",
    "from rfho.datasets import load_mnist, ExampleVisiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = load_mnist(partitions=(.2, .2)) # 20% of data in training set, 20% in validation \n",
    "# remaining in test set (you can change these percentages and see the effect on regularization hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = tf.placeholder(tf.float32, name='x'), tf.placeholder(tf.float32, name='y')\n",
    "# define the model (here use a linear model from rfho.models)\n",
    "model = rf.LinearModel(x, mnist.train.dim_data, mnist.train.dim_target)\n",
    "#vectorize the model, and build the state vector (augment by 1 since we are \n",
    "# going to optimize the weights with momentum) \n",
    "s, out, w_matrix = rf.vectorize_model(model.var_list, model.inp[-1], model.Ws[0],\n",
    "                                     augment=1)\n",
    "# (this function will print also some tensorflow infos and warnings about variables \n",
    "# collections... we'll solve this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define error \n",
    "error = tf.reduce_mean(rf.cross_entropy_loss(out, y))\n",
    "\n",
    "# define training error by error + L2 weights penalty\n",
    "rho = tf.Variable(0., name='rho') # regularization hyperparameter\n",
    "training_error = error + rho*tf.reduce_sum(tf.pow(w_matrix, 2))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# define learning rates and momentum factor as variables, to be optimized\n",
    "eta = tf.Variable(.03, name='eta')\n",
    "mu = tf.Variable(.9, name='mu')\n",
    "# now define the training dynamics (similar to tf.train.Optimizer)\n",
    "optimizer = rf.MomentumOptimizer.create(s, eta, mu, loss=training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to optimize the weights w.r.t. training_error\n",
    "# and hyperparameters w.r.t. validation error (that in this case is \n",
    "# error evaluated on the validation set)\n",
    "# we are going to use ReverseMode\n",
    "hyper_dict = {error: [rho, eta, mu]}\n",
    "hyper_grad = rf.ReverseHyperGradient(optimizer, hyper_dict) # this will calculate hyper-gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define helper for stochastic descent\n",
    "ev_data = ExampleVisiting(mnist, batch_size=200, epochs=20)\n",
    "tr_suppl = ev_data.create_train_feed_dict_supplier(x, y)\n",
    "val_supplier = ev_data.create_all_valid_feed_dict_supplier(x, y)\n",
    "test_supplier = ev_data.create_all_test_feed_dict_supplier(x, y)\n",
    "# all is set to compute the hyper-gradients. \n",
    "# now define optimizers for the hyperparameters and bounds \n",
    "# (we don't want rho, eta or mu to become negative..)\n",
    "hyper_optimizers = rf.create_hyperparameter_optimizers(hyper_grad, rf.AdamOptimizer)\n",
    "pos_constraints = rf.positivity(hyper_grad.hyper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run all for some hyper-iterations and print progresses \n",
    "with tf.Session().as_default() as ss:\n",
    "    tf.variables_initializer(hyper_grad.hyper_list).run() # initialize hyperparameters\n",
    "    [hy_opt.support_variables_initializer().run() for hy_opt in hyper_optimizers]\n",
    "    ev_data.generate_visiting_scheme()\n",
    "    for hyper_step in range(10):\n",
    "        hyper_grad.initialize() # reset weights to inital state\n",
    "        hyper_grad.run_all(T=ev_data.T, train_feed_dict_supplier=tr_suppl,\n",
    "                           val_feed_dict_suppliers=val_supplier) # optimize model and\n",
    "                                                                 # computes hyper-gradients\n",
    "        # apply hypergradients\n",
    "        [ss.run(h_optim.assign_ops) for h_optim in hyper_optimizers]\n",
    "        ss.run(pos_constraints)\n",
    "        \n",
    "        print('Concluded hyper-iteration', hyper_step)\n",
    "        print('Test accuracy:', ss.run(accuracy, feed_dict=test_supplier()))\n",
    "        print('Validation error:', ss.run(error, feed_dict=val_supplier()))\n",
    "        print('Values of hyperparameters')\n",
    "        [print(rf.simple_name(hyp), hyp.eval(), 'hyper-gradient:', g.eval()) for hyp, g in \n",
    "         zip(hyper_grad.hyper_list, hyper_grad.hyper_gradient_vars)]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
