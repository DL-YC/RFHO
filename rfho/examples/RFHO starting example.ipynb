{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Might want to install library \"tabulate\" for a better dictionary printing\nEnvironment variable RFHO_EXP_FOLDER not found. Current directory will be used\nExperiment save directory is  /Users/Riccardo/MEGA/TESI/RFHO/Experiments\nsklearn not found. Some load function might not work\nEnvironment variable RFHO_DATA_FOLDER not found. Variables HELP_WIN and HELP_UBUNTU contain info.\nData folder is /Users/Riccardo/MEGA/TESI/RFHO\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import rfho as rf\n",
    "\n",
    "from rfho.datasets import load_mnist, ExampleVisiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting /Users/Riccardo/MEGA/TESI/RFHO/mnist_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting /Users/Riccardo/MEGA/TESI/RFHO/mnist_data/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting /Users/Riccardo/MEGA/TESI/RFHO/mnist_data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting /Users/Riccardo/MEGA/TESI/RFHO/mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets.redivide_data:, computed partitions numbers - [0, 14000, 28000, 70000] len all 70000 DONE\n"
     ]
    }
   ],
   "source": [
    "mnist = load_mnist(partitions=(.2, .2)) # 20% of data in training set, 20% in validation \n",
    "# remaining in test set (you can change these percentages and see the effect on regularization hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    }
   ],
   "source": [
    "x, y = tf.placeholder(tf.float32, name='x'), tf.placeholder(tf.float32, name='y')\n",
    "# define the model (here use a linear model from rfho.models)\n",
    "model = rf.LinearModel(x, mnist.train.dim_data, mnist.train.dim_target)\n",
    "# vectorize the model, and build the state vector (augment by 1 since we are \n",
    "# going to optimize the weights with momentum) \n",
    "s, out, w_matrix = rf.vectorize_model(model.var_list, model.inp[-1], model.Ws[0],\n",
    "                                     augment=1)\n",
    "# (this function will print also some tensorflow infos and warnings about variables \n",
    "# collections... we'll solve this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define error \n",
    "error = tf.reduce_mean(rf.cross_entropy_loss(out, y))\n",
    "\n",
    "# define training error by error + L2 weights penalty\n",
    "rho = tf.Variable(0., name='rho') # regularization hyperparameter\n",
    "training_error = error + rho*tf.reduce_sum(tf.pow(w_matrix, 2))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# define learning rates and momentum factor as variables, to be optimized\n",
    "eta = tf.Variable(.01, name='eta')\n",
    "mu = tf.Variable(.5, name='mu')\n",
    "# now define the training dynamics (similar to tf.train.Optimizer)\n",
    "optimizer = rf.MomentumOptimizer.create(s, eta, mu, loss=training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'rfho' has no attribute 'ReverseHyperGradient'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-88647fd703a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# we are going to use ReverseMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhyper_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhyper_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReverseHyperGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_dict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# this will calculate hyper-gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'rfho' has no attribute 'ReverseHyperGradient'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# we want to optimize the weights w.r.t. training_error\n",
    "# and hyperparameters w.r.t. validation error (that in this case is \n",
    "# error evaluated on the validation set)\n",
    "# we are going to use ReverseMode\n",
    "hyper_dict = {error: [rho, eta, mu]}\n",
    "hyper_grad = rf.ReverseHyperGradient(optimizer, hyper_dict)  # this will calculate hyper-gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hyper_grad' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e92683218834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# now define optimizers for the hyperparameters and bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# (we don't want rho, eta or mu to become negative..)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhyper_optimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_hyperparameter_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpos_constraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositivity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyper_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hyper_grad' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# define helper for stochastic descent\n",
    "ev_data = ExampleVisiting(mnist, batch_size=200, epochs=20)\n",
    "tr_suppl = ev_data.create_train_feed_dict_supplier(x, y)\n",
    "val_supplier = ev_data.create_all_valid_feed_dict_supplier(x, y)\n",
    "test_supplier = ev_data.create_all_test_feed_dict_supplier(x, y)\n",
    "# all is set to compute the hyper-gradients. \n",
    "# now define optimizers for the hyperparameters and bounds \n",
    "# (we don't want rho, eta or mu to become negative..)\n",
    "hyper_optimizers = rf.create_hyperparameter_optimizers(hyper_grad, rf.AdamOptimizer)\n",
    "pos_constraints = rf.positivity(hyper_grad.hyper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run all for some hyper-iterations and print progresses \n",
    "with tf.Session().as_default() as ss:\n",
    "    tf.variables_initializer(hyper_grad.hyper_list).run()  # initialize hyperparameters\n",
    "    [hy_opt.support_variables_initializer().run() for hy_opt in hyper_optimizers]\n",
    "    ev_data.generate_visiting_scheme() # needed for remembering the example visited in forward pass\n",
    "    for hyper_step in range(20):\n",
    "        hyper_grad.initialize() # reset weights to initial state\n",
    "        hyper_grad.run_all(T=ev_data.T, train_feed_dict_supplier=tr_suppl,\n",
    "                           val_feed_dict_suppliers=val_supplier)  # optimize model and\n",
    "                                                                  # computes hyper-gradients\n",
    "        # apply hyper-gradients\n",
    "        [ss.run(h_optim.assign_ops) for h_optim in hyper_optimizers]\n",
    "        ss.run(pos_constraints)\n",
    "        \n",
    "        print('Concluded hyper-iteration', hyper_step)\n",
    "        print('Test accuracy:', ss.run(accuracy, feed_dict=test_supplier()))\n",
    "        print('Validation error:', ss.run(error, feed_dict=val_supplier()))\n",
    "        print('Values of hyperparameters')\n",
    "        [print(rf.simple_name(hyp), hyp.eval(), 'hyper-gradient:', g.eval()) for hyp, g in \n",
    "         zip(hyper_grad.hyper_list, hyper_grad.hyper_gradient_vars)]\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}